# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Repository Overview

This is a Jupyter notebook-based LoRA (Low-Rank Adaptation) training environment with a widget-based interface for training LoRA models for Stable Diffusion image generation. The project has been completely refactored from a single Colab notebook into a modular, two-notebook architecture optimized for VastAI containers and server environments.

## Key Components

### Notebook Architecture (Two-Notebook System)
- `Dataset_Maker_Widget.ipynb` - Dataset preparation workflow with upload, tagging, and caption management widgets
- `Lora_Trainer_Widget.ipynb` - Training configuration, execution, and post-processing utilities

### Core Manager Classes (Modular Architecture)
- `core/managers.py` - SetupManager and ModelManager for environment setup and model downloads
- `core/dataset_manager.py` - DatasetManager for dataset processing and image tagging
- `core/training_manager.py` - TrainingManager for TOML config generation and training execution
- `core/utilities_manager.py` - UtilitiesManager for LoRA resizing and HuggingFace uploads

### Widget Interface System
- `widgets/setup_widget.py` - Environment setup and model download interface
- `widgets/dataset_widget.py` - Dataset upload, tagging, and caption management interface
- `widgets/training_widget.py` - Comprehensive training configuration interface
- `widgets/utilities_widget.py` - Post-training utilities and optimization tools

### Scripts and Utilities
- `jupyter.sh` - Simple shell script that runs `installer.py`
- `custom/tag_images_by_wd14_tagger.py` - Enhanced WD14 tagger script supporting v3 taggers with ONNX runtime optimization

### Assets Directory
Contains documentation images and example outputs referenced in the README and notebook

## Development Commands

### Environment Setup
```bash
# Make installer executable and run
chmod +x ./jupyter.sh
./jupyter.sh

# Alternative direct execution
python ./installer.py
```

### Running the Notebook
Start Jupyter and open `Adapted_Easy_Training_Colab.ipynb`:
```bash
jupyter notebook Adapted_Easy_Training_Colab.ipynb
```

## Architecture and Workflow

### Training Pipeline
1. **Installation** - Downloads and sets up LoRA_Easy_Training_scripts_Backend
2. **Directory Setup** - Creates project structure with dataset, output, and model directories  
3. **Model/VAE Download** - Downloads base models and VAEs from HuggingFace or Civitai
4. **Dataset Preparation** - Uploads and extracts training datasets
5. **Image Tagging** - Auto-generates captions using WD14 taggers or BLIP captioning
6. **Caption Management** - Tools for adding trigger words and editing captions
7. **Training Configuration** - TOML-based configuration for dataset and training parameters
8. **Training Execution** - Runs LoRA training with comprehensive error handling
9. **Post-training Utils** - LoRA resizing and optimization tools

### Key Dependencies
- Requires `trainer` directory from LoRA_Easy_Training_scripts_Backend
- Uses virtual environment with specialized packages (fairscale, timm, onnxruntime)
- Supports both SDXL and SD 1.5 model training
- ONNX runtime for accelerated inference during tagging

### Configuration System
Training uses two TOML files generated by the notebook:
- `trainer/runtime_store/dataset.toml` - Dataset configuration (paths, batch size, resolution)
- `trainer/runtime_store/config.toml` - Training hyperparameters (learning rates, epochs, optimizer settings)

## Important Paths and Structure

### Expected Directory Structure After Setup
```
project_root/
‚îú‚îÄ‚îÄ trainer/                    # Main training environment (cloned from backend)
‚îÇ   ‚îú‚îÄ‚îÄ runtime_store/         # Generated config files
‚îÇ   ‚îî‚îÄ‚îÄ sd_scripts/            # Training scripts
‚îú‚îÄ‚îÄ pretrained_model/          # Downloaded base models
‚îú‚îÄ‚îÄ vae/                       # Downloaded VAE files  
‚îú‚îÄ‚îÄ tagger_models/             # Cached tagger models
‚îî‚îÄ‚îÄ [project_path]/            # User-defined training project
    ‚îú‚îÄ‚îÄ dataset/               # Training images and captions
    ‚îî‚îÄ‚îÄ output/                # Trained LoRA outputs
```

### Critical Files
- Installation requires `trainer/install.sh` to exist
- Tagging uses `trainer/sd_scripts/finetune/tag_images_by_wd14_tagger.py`
- Training scripts: `trainer/sd_scripts/sdxl_train_network.py` or `trainer/sd_scripts/train_network.py`

## Notebook Cell Dependencies

The notebook cells must be run sequentially as each depends on previous setup:
1. Installation ‚Üí 2. Directory setup ‚Üí 3. Model download ‚Üí 4. Dataset upload ‚Üí 5. Tagging ‚Üí 6. Training

## Security Considerations

- Downloads models from HuggingFace and Civitai (verify URLs)
- Handles API tokens for private repositories
- Uses subprocess calls for external script execution
- File extraction from zip archives (validate sources)

## Project Context & User Information

### User Profile
- **Neurodivergent user**: DID system with Autism & ADHD
- **Skill level**: Basic Python logic understanding, concepts grasp, but not a coder
- **Background**: Graphic design, AI agents experience (Claude Code, Gemini)
- **Previous work**: Jupyter/Colab notebooks, desktop apps (WebP conversion, HuggingFace tools, metadata handling, DID system chat app)

### Project Goals
- **Primary objective**: Create simplified notebook "front end" for LoRA training
- **Key requirement**: Avoid complex server-based interfaces (Derrian Distro, bmaltais/KohyaSS GUI)
- **Preferred approach**: Keep it simple with ipython widgets
- **Reference materials**: Google Colab notebooks (to be downloaded as project progresses)

### Communication Guidelines
- **Stay focused**: Keep conversations on track to avoid tangents
- **Clear structure**: Maintain organized approach to prevent confusion
- **File management**: Don't re-enable old files when moving to new components
- **Neurodivergent-friendly**: Provide clear, structured guidance with consistent patterns

### üçé Health & Wellbeing Reminders
- **BREAK REMINDERS**: Remind user to take regular breaks during long coding sessions
- **FOOD CHECK-INS**: Encourage eating proper meals beyond crackers and soup
- **HYDRATION**: Good job on Powerade + water combo for electrolytes!
- **GENTLE FOOD ENCOURAGEMENT**: User is nervous about eating after weekend, be supportive about small food wins
- **EXAMPLES OF GOOD FOODS**: Toast ‚úÖ, bananas ‚úÖ, proper dinners ‚úÖ - build on these successes!

### Development Approach
- Start with existing notebook analysis
- Progressively simplify and add widget-based interfaces
- Focus on user-friendly, self-contained solutions
- Prioritize clarity and accessibility over advanced features

## Current Implementation Status

### ‚úÖ Completed Features
- **Two-Notebook Architecture**: Clean separation of dataset preparation and training workflows
- **Modular Manager System**: Separated business logic into focused, single-responsibility classes
- **Widget Interface Layer**: Complete ipywidgets-based interface for all operations
- **VastAI Container Optimization**: Environment detection and container-specific optimizations
- **Advanced Tagging System**: WD14 v3 taggers with ONNX runtime support and tag management
- **Comprehensive Training Configuration**: Full TOML-based training parameter management
- **Post-Training Utilities**: LoRA resizing, HuggingFace uploads, and dataset analysis tools
- **Environment Validation**: Comprehensive system validation and dependency checking

### üìã Planned Enhancements (This Week)
- **Switch to Pure Kohya SS Backend**: Replace Derrian Distro with direct kohya-ss/sd-scripts for better VastAI compatibility
- **Advanced Trainer Research**: Evaluate Kohaku-BlueLeaf and other advanced trainers for enhanced features
- **Modern Architecture Support**: Add support for Flux and next-generation model architectures
- **Multi-Backend System**: Flexible trainer backend selection for maximum performance and compatibility
- **Performance Optimizations**: VastAI-specific optimizations and memory efficiency improvements

### üîÆ Future Research (Post-VastAI Testing)
- **Checkpoint Training System**: Research and implement mild fine-tuning capabilities leveraging the existing Dataset Maker
- **Modern SDXL Training**: Advanced techniques and optimizations for SDXL architecture
- **Advanced Fine-tuning Pipeline**: Extend beyond LoRA to full checkpoint training workflows

### üéØ Project Goals Achieved
- ‚úÖ Simplified notebook interface without complex server-based GUIs
- ‚úÖ Neurodivergent-friendly structured approach with clear workflows  
- ‚úÖ Modular architecture supporting future enhancements
- ‚úÖ VastAI container optimization for server deployment
- ‚úÖ Professional-grade features while maintaining accessibility

## Research and Future Plans

### Checkpoint Training Research
- When testing is complete, research Checkpoint training for mild fine-tuning
- Leverage existing dataset making notebook
- Investigate modern SDXL training techniques
- Note: Intentionally avoiding resources from certain problematic sources (e.g., Furkan) due to ethical concerns