{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img src=\"assets/doro.png\" width=\"32\" height=\"32\"> Dataset Maker - LoRA Training Assistant  \n",
    "\n",
    "This notebook contains the **Dataset Widget** for preparing your training datasets.\n",
    "\n",
    "## What this notebook handles:\n",
    "- **Dataset upload** and extraction from ZIP files\n",
    "- **Image tagging** with WD14 v3 taggers or BLIP captioning\n",
    "- **Caption management** and editing\n",
    "- **Trigger word** management and injection\n",
    "- **Tag filtering** and blacklist management\n",
    "\n",
    "## Workflow:\n",
    "1. **Upload your dataset** (ZIP file or folder)\n",
    "2. **Configure tagging** settings (WD14 for anime, BLIP for photos)\n",
    "3. **Review and edit** generated captions\n",
    "4. **Add trigger words** for your LoRA\n",
    "5. **Move to training** in `Lora_Trainer_Widget.ipynb`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <img src=\"assets/OTNDORODUSKFIXED.png\" width=\"32\" height=\"32\"> Dataset Preparation Widget\n",
    "\n",
    "This comprehensive widget handles all dataset preparation tasks:\n",
    "\n",
    "### üìÅ Dataset Upload & Management\n",
    "- **Local upload**: Drag & drop ZIP files or select folders\n",
    "- **HuggingFace import**: Direct download from HF datasets\n",
    "- **Automatic extraction**: Handles ZIP, RAR, 7z archives\n",
    "- **Folder organization**: Creates proper training structure\n",
    "\n",
    "### üè∑Ô∏è Auto-Tagging Systems\n",
    "\n",
    "#### WD14 v3 Taggers (Recommended for Anime/Art)\n",
    "- **wd14-vit-v2**: General purpose, balanced accuracy\n",
    "- **wd14-convnext-v2**: Higher accuracy, slower\n",
    "- **wd14-swinv2-v2**: Best for complex scenes\n",
    "- **ONNX optimization**: 2-3x faster inference\n",
    "- **Threshold control**: Adjust tag sensitivity\n",
    "\n",
    "#### BLIP Captioning (Best for Photos/Realistic)\n",
    "- **Natural language**: Generates descriptive sentences\n",
    "- **Scene understanding**: Captures context and relationships\n",
    "- **Perfect for**: Real photos, portraits, landscapes\n",
    "\n",
    "### ‚úèÔ∏è Caption Management\n",
    "- **Bulk editing**: Apply changes to all captions\n",
    "- **Find & replace**: Update specific terms across dataset\n",
    "- **Tag frequency analysis**: See most common tags\n",
    "- **Manual editing**: Individual caption refinement\n",
    "\n",
    "### üéØ Trigger Word System\n",
    "- **Automatic injection**: Add trigger words to all captions\n",
    "- **Position control**: Beginning, end, or random placement\n",
    "- **Consistency checking**: Ensure all images have triggers\n",
    "- **Preview mode**: See changes before applying\n",
    "\n",
    "### üö´ Tag Filtering & Blacklists\n",
    "- **Quality filters**: Remove low-confidence tags\n",
    "- **Content filters**: Block unwanted content types\n",
    "- **Custom blacklists**: Your own forbidden tags\n",
    "- **Whitelist mode**: Only allow specific tags\n",
    "\n",
    "### üìà Quality Analysis\n",
    "- **Image statistics**: Resolution, format, size analysis\n",
    "- **Tag distribution**: Most/least common tags\n",
    "- **Caption length**: Optimal length recommendations\n",
    "- **Duplicate detection**: Find similar images\n",
    "\n",
    "### üí° Best Practices Tips\n",
    "\n",
    "**Dataset Size:**\n",
    "- **Characters**: 15-50 images work well\n",
    "- **Styles**: 50-200 images for consistency\n",
    "- **Concepts**: 20-100 images depending on complexity\n",
    "\n",
    "**Image Quality:**\n",
    "- **Resolution**: 768x768 minimum, 1024x1024 recommended\n",
    "- **Variety**: Different poses, angles, expressions\n",
    "- **Consistency**: Similar art style or character design\n",
    "\n",
    "**Caption Quality:**\n",
    "- **Accuracy**: Verify auto-generated tags\n",
    "- **Completeness**: Include important details\n",
    "- **Trigger words**: Use unique, memorable terms\n",
    "- **Consistency**: Same terms for same concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from widgets.dataset_widget import DatasetWidget\n",
    "\n",
    "# Initialize and display the dataset widget\n",
    "dataset_widget = DatasetWidget()\n",
    "dataset_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  <img src=\"assets/doro_fubuki.png\" width=\"32\" height=\"32\"> Dataset Preparation Checklist\n",
    "\n",
    "Before moving to training, ensure you have:\n",
    "\n",
    "### ‚úÖ Dataset Structure\n",
    "- [ ] Images are in a single folder\n",
    "- [ ] All images have corresponding .txt caption files\n",
    "- [ ] No corrupted or unreadable images\n",
    "- [ ] Consistent image format (jpg/png)\n",
    "\n",
    "### ‚úÖ Caption Quality\n",
    "- [ ] All captions contain your trigger word\n",
    "- [ ] Tags are accurate and relevant\n",
    "- [ ] No unwanted or problematic tags\n",
    "- [ ] Caption length is reasonable (50-200 tokens)\n",
    "\n",
    "### ‚úÖ Content Verification\n",
    "- [ ] Images represent what you want to train\n",
    "- [ ] Sufficient variety in poses/angles\n",
    "- [ ] Consistent quality across dataset\n",
    "- [ ] No duplicate or near-duplicate images\n",
    "\n",
    "---\n",
    "\n",
    "##  <img src=\"assets/OTNANGELDOROFIX.png\" width=\"32\" height=\"32\"> Next Steps\n",
    "\n",
    "Once your dataset is prepared:\n",
    "\n",
    "1. **Note your dataset path** - you'll need it for training\n",
    "2. **Remember your trigger word** - important for generation\n",
    "3. **Open** `Lora_Trainer_Widget.ipynb` for training setup\n",
    "4. **Run the Setup widget** first in the training notebook\n",
    "\n",
    "---\n",
    "\n",
    "## <img src=\"assets/OTNEARTHFIXDORO.png\" width=\"32\" height=\"32\"> Troubleshooting\n",
    "\n",
    "### Common Issues\n",
    "\n",
    "**\"No images found\":**\n",
    "- Check ZIP file structure (images should be in root or single folder)\n",
    "- Verify image formats (jpg, png, webp supported)\n",
    "- Ensure files aren't corrupted\n",
    "\n",
    "**\"Tagging failed\":**\n",
    "- Check internet connection for model downloads\n",
    "- Verify sufficient disk space (2-3GB for tagger models)\n",
    "- Try different tagger model\n",
    "\n",
    "**\"Captions too long/short\":**\n",
    "- Adjust tag threshold settings\n",
    "- Use tag filtering to remove excess tags\n",
    "- Consider manual editing for important images\n",
    "\n",
    "**\"Missing trigger words\":**\n",
    "- Use bulk edit to add trigger words\n",
    "- Check trigger word injection settings\n",
    "- Verify trigger word isn't being filtered out\n",
    "\n",
    "---\n",
    "\n",
    "*Ready to create amazing LoRAs? Let's go! üöÄ*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
